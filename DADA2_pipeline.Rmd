---
title: "DADA2 Pipeline"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
author: "Briana K. Whitaker"
date: "`r Sys.Date()`"
---
\fontsize{9}{10}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width=3.5, fig.height=3,
                      warning=FALSE, message=FALSE)
```
---

* Run using `r version[['version.string']] `.

# Objective
This document reports on the **bioinformatics analysis** of the Variety Sampling mycobiome data from samples collected in 2020 and 2021 in Ewing and Urbana Illinois. 


# 0) Load Packages, set path to data
```{r load, echo=FALSE, results='hide', include=FALSE} 
x<-c("BiocManager", "dada2", "ShortRead", "Biostrings", "seqinr", "tidyverse",
     "ggplot2", "phyloseq", "DECIPHER")
lapply(x, require, character.only = TRUE)


#add 'not in' function
`%nin%` = Negate(`%in%`)

#set seed
set.seed(639)

# set ggplot2 theme
theme_set(theme_bw(base_size=16)) 
theme_update(panel.grid.major=element_line(0), panel.grid.minor=element_line(0))

# set path for zipped and deindexed fastq files
path <- "./RawSq"  #for round1 of sequencing (~1/2 of samples)
list.files(path)
# 488 files

#make a list of matched sample names
fnFs <- sort(list.files(path, pattern = "R1_001.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "R2_001.fastq.gz", full.names = TRUE))
# check lengths
length(fnFs); length(fnRs)  #244 files each


get.Sample.ID <- function(fname) strsplit(basename(fname), "_")[[1]][1]  
ids.dat <- as.data.frame(cbind("num" = unname(sapply(fnFs, get.Sample.ID)),
                               fnFs, fnRs))
Sample.ID <- ids.dat$num
sort(Sample.ID)
#tableOfSampleIDs <- sort(table(Sample.ID))
```

* Negatives: "PCR1-Blk-A" "PCR1-Blk-B" "PCR1-Blk-C" "PCR2-Blank"


```{r loopCount, results='hide', echo=FALSE, include = FALSE}
# #loop to count the number of seqs originally
# i = 1 #useful for checking
# fwdSeqs <- list()
# revSeqs <- list()
# for (i in 1:length(fnFs)) {
#  fwdSeqs[[i]] <- length(sapply(fnFs[i], getSequences))
#  revSeqs[[i]] <- length(sapply(fnRs[i], getSequences))
# }
# identical(c(unlist(fwdSeqs)),c(unlist(revSeqs))) #TRUE
# SeqsOrig.df <- data.frame(SampleID = c(basename(fnFs)) ,
#           OrigSeqsFwd = c(unlist(fwdSeqs)),  OrigSeqsRev = c(unlist(revSeqs)))
# rownames(SeqsOrig.df) <- SeqsOrig.df$SampleID
# SeqsOrig.df <- SeqsOrig.df[,-1]
# write.csv(SeqsOrig.df, "./intermediate/VarSamp_TrackSequences_PriorFiltering.csv")
SeqsOrig.df <- read.csv("./intermediate/VarSamp_TrackSequences_PriorFiltering.csv",
                        row.names = 1)
```


# 1) Initial Filter Step
```{r initFilt, results='hide'}
# filter out reads with ambiguous bases (N) only
# Put N-filterd files in filtN/ subdirectory
fnFs.filtN <- file.path(path, "filtN-", basename(fnFs)) 
fnRs.filtN <- file.path(path, "filtN-", basename(fnRs))

#filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0)
```

```{r orientsFunc, results='hide', echo=FALSE}
#identify primers used, including ambiguous bases
FWD <- "GTGARTCATCGAATCTTTG" #fITS7
REV <- "TCCTSCGCTTATTGATATGC" #ITS4ngs

#check that we have the right orientation of both primers
allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  #Biostrings needs DNAString objects
    orients <- c(Forward = dna, Complement = complement(dna), Reverse = reverse(dna), 
        RevComp = reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
# return orientations
FWD.orients <- allOrients(FWD);FWD.orients
REV.orients <- allOrients(REV);REV.orients

#count no. times primers appear (and orientations), for 1 file only as representative
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
```

```{r checkPrim}
# read this table as -- the column headers indictaing the direction of the primer
# (i.e., forward direction of either the FWD or REV primer)
# and the rownames indicating the combo of primer (FWD/REV) and read type (Forward/Reverse)
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))
#D01-T1
#                  Forward Complement Reverse RevComp
# FWD.ForwardReads   79442          0       0       0
# FWD.ReverseReads       0          0       0   40253
# REV.ForwardReads       0          0       0   63237
# REV.ReverseReads   79922          0       0       0

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[145]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.filtN[[145]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.filtN[[145]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[145]]))
#L01-T1
#                  Forward Complement Reverse RevComp
# FWD.ForwardReads    5164          0       0       1
# FWD.ReverseReads       1          0       0    3191
# REV.ForwardReads       0          0       0    4180
# REV.ReverseReads    5212          0       0       0
```

# 2) Remove Primers
```{r cutadaptSetup, results='hide', echo=FALSE}
cutadapt <- "C:/path/to/cutadapt.exe" #change to location on your machine
system2(cutadapt, args = "--version") #v.4.0

path.cut <- file.path(path, "cutadapt-")
#if (!dir.exists(path.cut)) dir.create(path.cut)
fnFs.cut <- file.path(path.cut, basename(fnFs))
fnRs.cut <- file.path(path.cut, basename(fnRs))


#define reverse complements of each primer (FOR LATER)
FWD.RC <- dada2:::rc(FWD)
REV.RC <- dada2:::rc(REV)
```
```{r cutadaptRun, results='hide'}
# add flags for cutadapt command
R1.flags <- paste("-g", FWD, "-a", REV.RC) #-g for 5' end, -a for 3' end
R2.flags <- paste("-G", REV, "-A", FWD.RC) 

# #Run Cutadapt
#for (i in seq_along(fnFs)) {
#  system2(cutadapt, args = c(R1.flags, R2.flags, "-n", 2, "-m", 50, "-e", 0.1,
#              "-o", fnFs.cut[i], "-p", fnRs.cut[i],
#                    fnFs.filtN[i],     fnRs.filtN[i] ))   }
```

```{r checkPrim2}
#sanity check, see if primers were removed from 1st sample
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[1]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[1]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[1]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[1]]))
# all 0's

rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.cut[[145]]), 
    FWD.ReverseReads = sapply(FWD.orients, primerHits, fn = fnRs.cut[[145]]), 
    REV.ForwardReads = sapply(REV.orients, primerHits, fn = fnFs.cut[[145]]), 
    REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.cut[[145]]))
#                  Forward Complement Reverse RevComp
# FWD.ForwardReads       0          0       0       1
# FWD.ReverseReads       1          0       0       0
# REV.ForwardReads       0          0       0       0
# REV.ReverseReads       0          0       0       0


#get filenames of cutadapt-ed files
cutFs <- sort(list.files(path.cut, pattern = "R1_001.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "R2_001.fastq.gz", full.names = TRUE))
 
#Sample.ID
```

### Inspect Quality Plots

```{r qualF, warning=FALSE, message=FALSE, results='hide', echo=FALSE, fig.height=4, fig.width =4}
# #inspect read quality plots
# pdf("figures/VarSamp-SequenceQuality.pdf",
#     width=16/2.54, height=16/2.54)
plotQualityProfile(cutFs[ c(1:2,145:146)])
```

-

```{r qualR, warning=FALSE, message=FALSE, results='hide', echo=FALSE, fig.height=4, fig.width =4}
plotQualityProfile(cutRs[c(1:2,145:146)])
#dev.off()
```


# 3) Filter and Trim

```{r filtPath, results='hide', echo = FALSE}
#set filenames for creating filtered files from cutadapt-ed files
filtFs <- file.path(path, "filtered-final", basename(cutFs))
filtRs <- file.path(path, "filtered-final", basename(cutRs))

```

*Results of tests of maxEE parameter in second filtering:* 
```{r secondFilt, results='hide'}
# #perform second filtering, keep maxN=0   #8min
# out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0,
#                     maxEE = c(2, 2),
#    truncQ = 2, minLen = 50, rm.phix = TRUE, compress = TRUE)
# save(out, file="./intermediate/VarSamp_FilterOut.RData")
load("./intermediate/VarSamp_FilterOut.RData")
```

### Inspect the No. of reads, before and after 2nd filtering step
```{r inspectFilt}
head(out)
#sum(out[,2])/sum(out[,1])
```

* Looks like about ~30% of reads are being lost in debris samples, bit more variable for leaves.

```{r writeout, results='hide', echo=FALSE}
# write.csv(out, "./intermediate/VarSamp-SecondFilteringStep.csv")
# out <- read.csv("./intermediate/VarSamp-SecondFilteringStep.csv", row.names=1)

```


# 4) Learn Errors, Dereplicate, & Denoise

### Learn Errors
```{r learnerrSetup, results='hide', echo=FALSE}
# to continue with pipeline, ignoring samples that don't pass the filter
not.lost <- out[,"reads.out"] > 0  
length(table(not.lost))  #2 samples reduced to 0 reads

filtFs <- filtFs[not.lost]
filtRs <- filtRs[not.lost]
Sample.ID2 <- Sample.ID[not.lost]

```

* PCR1-Blk-B and L12-T3 reduced to 0 reads

```{r learnerr, results='hide'}
# #The DADA2 algorithm makes use of a parametric error model (err),
# errF <- learnErrors(filtFs) #used 7 samples to learn
# errR <- learnErrors(filtRs)
# save(errF, file="./intermediate/VarSamp_errF.RData")
# save(errR, file="./intermediate/VarSamp_errR.RData")
load("./intermediate/VarSamp_errF.RData")
load("./intermediate/VarSamp_errR.RData")

```

#### Plant Error Models
```{r plotErrF, results='hide', echo=FALSE}
# #sanity check, plot errors
# pdf("figures/VarSamp-ErrorLearning.pdf",
#      width=16/2.54, height=16/2.54)
plotErrors(errF, nominalQ=TRUE)  
```

-

```{r plotErrR, results='hide', echo=FALSE}
plotErrors(errR, nominalQ=TRUE)
#dev.off()
```

* Forward read patterns look a little odd.

### Dereplicate

```{r derep, results='hide'}
# #dereplicate identical reads into unique reads (with an abundance/count value)
# derepFs <- derepFastq(filtFs, verbose=TRUE)
# derepRs <- derepFastq(filtRs, verbose=TRUE)
# # Name the derep-class objects by the sample names
# names(derepFs) <- Sample.ID2
# names(derepRs) <- Sample.ID2

```
```{r inspectDerep, results='hide', echo=FALSE}
# save(derepFs, file="./intermediate/VarSamp_derepFs.RData")
# save(derepRs, file="./intermediate/VarSamp_derepRs.RData")
load("./intermediate/VarSamp_derepFs.RData")
load("./intermediate/VarSamp_derepRs.RData")
derepFs[1] #example

```

### Denoise

```{r denoise, results='hide'}
# #core denoising algorithm
# #is built on the parametric error model inferred directly from reads.
# dadaFs <- dada(derepFs, err=errF)
# dadaRs <- dada(derepRs, err=errR)
# save(dadaFs, file="./intermediate/VarSamp_dadaFs.RData")
# save(dadaRs, file="./intermediate/VarSamp_dadaRs.RData")
load("./intermediate/VarSamp_dadaFs.RData")
load("./intermediate/VarSamp_dadaRs.RData")
dadaFs[1] #example

```

# 5) Make Contigs

```{r merge, results='hide'}
# #merge fwd and rev reads together, i.e. contigs     #2mins
# mergers <- mergePairs(dadaFs, derepFs, 
#                       dadaRs, derepRs, verbose = TRUE) #additional arg. minOverlap
# save(mergers, file="./intermediate/VarSamp_Mergers.RData")
load("./intermediate/VarSamp_Mergers.RData")

```

```{r inspectMerge, results='hide', echo=FALSE}
# Inspect the merger data.frame from the first sample
head(mergers[[1]])

#make amplicon sequence variant table (ASV) table
seqtab <- makeSequenceTable(mergers)
#dim(seqtab)
# # [1]  242 4678  
# save(seqtab, file="./intermediate/VarSamp_seqtab.RData") 
load("./intermediate/VarSamp_seqtab.RData")

```

### Get a sense of contig length variation
```{r lengthVar, results='hide', echo=FALSE}
#table(nchar(getSequences(seqtab))) #52-566bp
med.seqtab <- median(nchar(getSequences(seqtab)))
med.seqtab

hist(nchar(getSequences(seqtab)), main = "Seq. Length")
abline(v= med.seqtab, lty=2, col='red', lwd=3) 

```

* Median basepair length is 249

# 6) Chimera checking
```{r chimera, results='hide'}
# identify chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", verbose=TRUE)
#Identified 1777 bimeras out of 4678 input sequences.

```


### Freq. of chimeric sequences
```{r propChimera}
sum(seqtab.nochim)/sum(seqtab)
```

* ~4.5% chimeras by adundance

```{r writeChimera, results='hide', echo=FALSE}
dim(seqtab.nochim)
#[1]  242 2901
# save(seqtab.nochim, file="./data/VarSamp_seqtab.nochim.RData")
# write.csv(seqtab.nochim, "./data/VarSamp_SbyS.csv")

```

# 7) Track Reads

**(hidden)**

```{r trackReads, results='hide', echo=FALSE}
# #subset again to remove any samples that did not pass filtering
# SeqsOrig.df2 <- SeqsOrig.df[not.lost,]
# out2 <- out[not.lost,]

# track reads through the pipeline
getN <- function(x) sum(getUniques(x))

# # track reads through the pipeline
# track <- cbind(SeqsOrig.df2[,1],
#              out2[,1],
#              round(out2[,1]/SeqsOrig.df2[,1]*100, 2),
#              out2[,2],
#              round(out2[,2]/out2[,1]*100,2),
#              sapply(dadaFs, getN), sapply(dadaRs, getN),
#              round(sapply(dadaFs, getN)/out2[,2]*100,2),
#              sapply(mergers, getN),
#              round(sapply(mergers, getN)/sapply(dadaFs, getN)*100,2),
#              rowSums(seqtab.nochim),
#              round(rowSums(seqtab.nochim)/sapply(mergers, getN)*100,2) )
# 
# colnames(track) <- c("OrigSeqsF", "post1stFilter",
#  "PercKept1stFilter", "post2ndFilter", "PercKept2ndFilter","denoisedF",
#  "denoisedR", "PercKeptDenoise", "Merged", "PercKeptMerge",
#  "postChimera","PercKeptChimera")
# rownames(track) <- Sample.ID2
# head(track)
# write.csv(track, "./intermediate/VarSamp_TrackSequences.csv")
track <- read.csv("./intermediate/VarSamp_TrackSequences.csv", 
                  row.names = 1)

table(track[,"postChimera"]<5000)
```

* 31 samples <1000 reads. 
* 87 samples <5K reads.
* Lost a lot of reads in the 2nd filter
* The higher chimera rates are in the debris samples (worst) and head samples (particularly the T5 samples = worst), almost no losses in the leaf samples. Which is interesting.... go back and double check the trial illumina runs

* 1 neg control had no reads pass 2nd filter, rest of negative controls <432 reads,
* 14 samples < 432 reads, with additional 14 samples < 1000 reads.


# 8) Insepct Sequencing Run

```{r mergeRuns, results='hide', echo=FALSE}
# load 
load("./data/VarSamp_seqtab.nochim.RData")
dim(seqtab.nochim)

```


```{r makephyloseq, results='hide', echo=FALSE}
## Plot Data
disease <- read.csv("./data/VarietySampling_Plot_Data_2022-12-27.csv")
disease$site <- as.factor(disease$site)  
disease$manager <- as.factor(disease$manager)                    
disease$managerPlotName <- as.factor(disease$managerPlotName)
disease$block <- as.factor(disease$block)
disease$FHBresistance <- as.factor(disease$FHBresistance)
disease$variety <- as.factor(disease$variety)
disease$plotID <- as.factor(unname(sapply(
    disease$plotID, function(x) strsplit(basename(x), "_")[[1]][2])))
disease %>% select(-species, -Lat.deg, -Long.deg, -harvestMethod) -> disease

## Sample Data
SbyE0 <- read.csv("./data/VarietySampling_Illumina_2023-07-13.csv")
#dim(SbyE0)
rownames(SbyE0) <- SbyE0$ID
#str(SbyE0)
SbyE0$ID <- factor(SbyE0$ID); SbyE0$typeSample <- factor(SbyE0$typeSample);
SbyE0$typeCode <- factor(SbyE0$typeCode); 
SbyE0$timeSample <- factor(SbyE0$timeSample); SbyE0$DNA_Col <- factor(SbyE0$DNA_Col);
SbyE0$DiseaseStatus <- factor(SbyE0$DiseaseStatus);
SbyE0$manager <- factor(SbyE0$manager); SbyE0$variety <- factor(SbyE0$variety);
SbyE0$DNA_Row <- factor(SbyE0$DNA_Row); SbyE0$DNA_Plate <- factor(SbyE0$DNA_Plate);
SbyE0$DNA_Location <- factor(SbyE0$DNA_Location)
SbyE0$plotID <- as.factor(unname(sapply(
    SbyE0$plotID, function(x) strsplit(basename(x), "_")[[1]][2])))
SbyE0 %>% select(-uLSample_normalize, -uLWater_normalize, -QuantIt_ngperuL) -> SbyE0
# remove samples reduced to 0 reads after filter
SbyE0 %>% filter(ID != "PCR1-Blk-B") %>% filter(ID != "L12-T3") -> SbyE0b
# sanity check, must be true
identical(sort(rownames(SbyE0b)), sort(rownames(seqtab.nochim)))

## merge two datasets
merge(SbyE0b, disease, by = "plotID", all.x = TRUE) -> SbyE
rownames(SbyE) <- SbyE$ID
#write.csv(SbyE, "./data/VarSamp_SbyE.csv")
dim(SbyE)


# ## Phyloseq object
# ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
#                     sample_data(SbyE))
# dna <- Biostrings::DNAStringSet(taxa_names(ps))
# names(dna) <- taxa_names(ps)
# ps <- merge_phyloseq(ps, dna)
# taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
# ps
# #output my own fasta file
# writeXStringSet(refseq(ps),
#                 filepath = "./data/VarSamp_uniqueSeqs.fasta")
# uniqueSS <- readDNAStringSet("./data/VarSamp_uniqueSeqs.fasta")
# # save ps object
# save(ps, file="./data/VarSamp_ps.RData")

load("./data/VarSamp_ps.RData")
ps

```

* Number of unique ASVs: `r length(getUniques(seqtab.nochim))`



```{r session}
sessionInfo()
```



##### end